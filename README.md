# Use of AI for Literature Review

**A 5-Day Intensive Course on AI-Assisted Research**

*IAEA Laboratories, Seibersdorf, Austria | October 2025*

## Course Overview

This hands-on course explores how to effectively integrate AI and Large Language Models (LLMs) into literature review workflows. Rather than treating AI as a magic solution, we focus on developing a methodical, intentional approach that keeps researchers in control while enhancing their capabilities.

### Philosophy: AI as a Tool for Intentional Research

- AI as a partner for systematic research, not a replacement for critical thinking
- Focus on methodology over specific tools
- Emphasis on evaluation, reliability, and maintaining research quality
- Intentional approach: you drive, AI assists

## What You'll Learn

### Core Topics
- **LLM Fundamentals**: Understanding how these systems work and why they sometimes fail
- **Advanced Prompt Engineering**: Building effective prompts through systematic, iterative refinement
- **Evaluation & Quality Control**: Systematic approaches using open/axial coding and LLM-as-judge methodology
- **Multi-Paper Synthesis**: Intentional workflows for literature review with IP awareness
- **Real-World Applications**: Case studies and adaptation to your research domain
- **Ethics & IP Constraints**: Respecting copyright while leveraging AI capabilities

## 5-Day Schedule

### Day 1: Foundations & Initial Experience
**Morning:**
- Course philosophy and scope
- Tools exploration: "Who's using what?" (collaborative session)

**Afternoon:**
- First hands-on: Try paper extraction yourself (authentic experience)
- Debrief and introduction to the Three Gulfs Model
- Collaborative discussion: expectations, concerns, and ethical considerations

**Goal:** Experience challenges before learning frameworks

### Day 2: Prompt Engineering Fundamentals
**Morning:**
- How LLMs are trained: understanding capabilities and limitations
- The seven building blocks of effective prompts

**Afternoon:**
- Iterative hands-on: Build paper extraction prompt step-by-step
- Apply each building block through cycles
- Group comparison and discussion of different approaches

**Goal:** Develop systematic paper extraction workflows

### Day 3: Evaluation Methodology
**Morning:**
- Why LLM evaluation is not optional
- Evaluation methodology: open coding, axial coding, and LLM-as-judge concepts

**Afternoon:**
- Hands-on: Toy example (plagiarism detection)
- Live axial coding session: analyzing patterns from shared observations
- Apply evaluation to your Day 2 prompts

**Goal:** Move from "vibes" to systematic assessment

### Day 4: Synthesis & Hands-On Practice
**Morning:**
- Discussion: Your current literature review practices
- Multi-paper synthesis prompt building with IP constraints
- Live demo: 20-paper literature review workflow in SolveIt

**Afternoon:**
- Hands-on: Apply and refine synthesis prompts with real papers
- Explore what's possible with your preferred tools

**Goal:** Apply intentional synthesis to real literature review

### Day 5: Real-World Applications, Ethics & Future
**Morning:**
- Real-world case studies:
  - MARIS RAG system (Monaco): IP-aware synthesis at scale
  - IOM report evaluation: Multi-step prompts in action
- Collaborative synthesis: Documenting our collective learning

**Afternoon:**
- Open office hours: Individual lab consultations
- Planning AI workflows for your research
- Next steps and 6-day support planning

**Goal:** Transition from course to practice

## Course Approach

- **Experience First**: Start with authentic challenges before introducing frameworks
- **Iterative Learning**: Build complexity gradually through hands-on cycles
- **Collaborative**: Shared documentation and group learning throughout
- **Methodological**: Develop systematic approaches, not ad-hoc tool use
- **Honest**: Acknowledge limitations and share real development processes
- **Intentional**: You drive the research, AI assists

## Course Objectives

By the end of this course, you will:

- Understand systematic methodology for AI-assisted literature review
- Have practical experience building and evaluating prompts
- Know evaluation approaches: open/axial coding and LLM-as-judge
- Understand IP-aware synthesis techniques
- Have developed an ethical framework for AI use in research
- Possess refined prompts for paper extraction and synthesis tailored to your domain
- Contribute to a collaborative report documenting our collective approach
- Have a plan for implementing AI workflows in your research (with 6 days follow-up support)


## Prerequisites

- Basic familiarity with research and literature review processes
- No prior AI or programming experience required
- Bring your own research questions and papers to work with
- Access to at least one AI tool (ChatGPT, Claude, Copilot, etc.)

## What You'll Take Away

- A systematic methodology for AI-assisted literature review
- Practical experience building and evaluating prompts
- Understanding of evaluation approaches (open/axial coding, LLM-as-judge)
- Knowledge of IP-aware synthesis techniques
- Ethical framework for AI use in research
- Your own refined prompts for paper extraction and synthesis
- Individual consultation opportunities (6 additional days available)

## Materials Provided

### Presentations
- Course scope and philosophy
- LLM training fundamentals
- Prompt engineering principles
- The Three Gulfs Model
- Evaluation methodologies (open/axial coding, LLM-as-judge)
- Multi-paper synthesis and IP constraints
- Real-world case studies

### Hands-On Guides
- Session 1C: First paper extraction attempt
- Session 2C: Iterative prompt building (cycles through building blocks)
- Session 3B: Evaluation toy example (plagiarism detection)
- Session 3C: Evaluating your own prompts
- Session 4B: Multi-paper synthesis workflow

### Reference Materials
- Prompt engineering fundamentals guide
- Paper extraction prompt template
- IP-aware synthesis guidelines (MARIS approach)
- Evaluation methodology overview

### Shared Resources
- Google Docs for collaborative observations and discussions
- Toy datasets for evaluation practice
- Example prompts and case studies

## Target Audience

Researchers, graduate students, and professionals who want to:
- Enhance their literature review capabilities with AI
- Understand AI tools beyond the hype
- Develop reliable, ethical approaches to AI-assisted research
- Stay in control while leveraging AI capabilities
- Learn systematic evaluation methodologies
- Apply intentional approaches to their research domains

## Presentations

Available in the `presentations/` directory:
- Course scope and philosophy
- LLM training fundamentals
- Prompt engineering principles
- The Three Gulfs Model
- Evaluation methodologies

## Getting Started with Quarto/Reveal.js

This course uses Quarto with Reveal.js for interactive presentations.

### Setup
To use the same template as the course materials:
```bash
quarto use template grantmcdermott/quarto-revealjs-clean-demo
```

### Viewing Presentations
To render and view a presentation file:
```bash
quarto preview my-presentation.qmd
```
This will open the presentation in your local browser with live reload.

### Converting to PDF
Built-in print stylesheet (recommended): Quarto reveal.js presentations can be exported to PDF via a special print stylesheet. Simply add `?print-pdf` to your presentation URL, open the browser print dialog (Ctrl+P), change destination to "Save as PDF", set layout to landscape, margins to none, and enable background graphics.

## About the Instructor

**Franck Albinet**  
Independent Data Science & AI Consultant  
Email: franckalbinet@gmail.com

---

*Course materials updated October 2025. For questions or early access inquiries, contact the instructor.*